{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ea3c82f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'datadetox' from 'c:\\\\Users\\\\USER\\\\Desktop\\\\GitHub\\\\Brasileirao-Analise\\\\3. Source Code\\\\Transformed\\\\datadetox.py'>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install -U torch transformers accelerate sentence-transformers\n",
    "# !pip install -U torch transformers accelerate sentence-transformers\n",
    "# !pip uninstall spellchecker -y\n",
    "# !pip install pyspellchecker\n",
    "# !pip install unidecode\n",
    "# !pip install scikit-learn  # For cosine_similarity and numpy (it's a dependency)\n",
    "# !pip install sentence-transformers\n",
    "# !pip install pandas\n",
    "#!pip install ipywidgets\n",
    "\n",
    "import re\n",
    "import unidecode\n",
    "from spellchecker import SpellChecker\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datadetox as detox\n",
    "import importlib\n",
    "importlib.reload(detox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be09036e",
   "metadata": {},
   "source": [
    "Leitura e padronização de colunas de chaves primarias para concatenação de dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7a17ab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 0: Já é datetime64, mantido como está.\n",
      "DataFrame 1: Convertendo padrão com '/'\n",
      "DataFrame 2: Convertendo padrão com '/'\n",
      "DataFrame 3: Convertendo padrão com '/'\n",
      "DataFrame 4: Já é datetime64, mantido como está.\n",
      "DataFrame 5: Já é datetime64, mantido como está.\n",
      "DataFrame 6: Convertendo padrão ISO com '-'\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "path = r'C:\\Users\\USER\\Desktop\\GitHub\\Brasileirao-Analise\\1. Data\\Transformed'\n",
    "\n",
    "#Games \n",
    "df_list = []\n",
    "for i in range(1, 8):\n",
    "    df = pd.read_excel(path + f'\\\\0{i}_games.xlsx')\n",
    "    df_list.append(df)\n",
    "\n",
    "#fitragem série B comjumto 7\n",
    "df_list[6] = df_list[6][df_list[6]['Série']=='A']\n",
    "\n",
    "# mapeamento_colunas chave primária\n",
    "mapeamento_colunas = {  'data': 'Data','Date': 'Data','data_partida': 'Data',\n",
    "                        'time_mandante': 'Mandante','mandante': 'Mandante','Home': 'Mandante',\n",
    "                        'time_visitante': 'Visitante','visitante': 'Visitante','Away': 'Visitante'}\n",
    "    \n",
    "\n",
    "df_list = [df.rename(columns=mapeamento_colunas) for df in df_list]\n",
    "\n",
    "df_list = detox.padronizar_datas(df_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636f798d",
   "metadata": {},
   "source": [
    "Unir todos os DFs com agrupamento por colunas, Data, Mandante e Visitante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16a3459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapear cluber qeu o filtro BERT nao resolve (manualmente)\n",
    "clubes_map_inicial = {  \"America-RN\":\"América Natal\",\"América-RN\":\"América Natal\",\"América rn\":\"América Natal\",\n",
    "                        \"Joinvile\":\"Joinville\",\n",
    "                        \"Atlético pr\":\"Athlético-PR\",\"Atlético-PR\":\"Athlético-PR\", \n",
    "                        \"Grêmio Prudente\":\"Barueri\",\"Grêmio prudente\":\"Barueri\",\"Gremio Prudente\":\"Barueri\"}\n",
    "\n",
    "for df in df_list:\n",
    "    df['Mandante'] = df['Mandante'].replace(clubes_map_inicial)\n",
    "    df['Visitante'] = df['Visitante'].replace(clubes_map_inicial)\n",
    "\n",
    "# listar clubes Mandantes e visitantes, para agrupar ortograficamente\n",
    "clubes_unicos = []\n",
    "for df in df_list:\n",
    "    unicos_por_bd = pd.unique(np.concatenate([df['Mandante'].values, df['Visitante'].values])).tolist()\n",
    "    clubes_unicos = list(set(clubes_unicos) | set(unicos_por_bd))\n",
    "\n",
    "clubes_value_dict = [   \"América Mineiro\", \"América Natal\", \"Athletico Paranaense\", \"Atlético Goianiense\", \"Atlético Mineiro\", \"Avaí\", \"Bahia\", \n",
    "                        \"Barueri\", \"Botafogo\", \"Red Bull Bragantino\", \"Brasiliense\", \"Ceará\", \n",
    "                        \"Chapecoense\", \"Corinthians\", \"Coritiba\", \"Criciúma\", \"Cruzeiro\", \"CSA\", \"Cuiabá\", \"Figueirense\", \"Flamengo\", \"Fluminense\", \n",
    "                        \"Fortaleza\",\"Guarani\", \"Goiás\", \"Grêmio\", \"Grêmio Prudente\", \"Internacional\", \"Ipatinga\", \"Joinville\", \"Juventude\",\"Mirassol\", \"Náutico\", \n",
    "                        \"Palmeiras\", \"Paraná\", \"Paysandu\", \"Ponte Preta\", \"Portuguesa\", \"Santa Cruz\", \"Santo André\", \"Santos\", \"São Caetano\", \"São Paulo\", \n",
    "                        \"Sport Recife\", \"Vasco da Gama\", \"Vitória\"]\n",
    "\n",
    "modelo = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "clubes_corrigidos = detox.corrigir_times_semanticamente(clubes_unicos, clubes_value_dict,modelo)\n",
    "clubes_dict = detox.gerar_dicionario_mapeamento(clubes_corrigidos, limite=0.0)\n",
    "\n",
    "# corrigindo e agrupando clubes em todos os dataframes\n",
    "for df in df_list:\n",
    "    df['Mandante'] = df['Mandante'].replace(clubes_dict)\n",
    "    df['Visitante'] = df['Visitante'].replace(clubes_dict)\n",
    "\n",
    "# unindo todos os dfs com merge on=['Data','Mandante','Visitante']\n",
    "df_merged = pd.DataFrame(columns=['Data','Mandante','Visitante'])\n",
    "for i,df in enumerate(df_list):\n",
    "    df_merged = pd.merge(df_merged, df, how='outer', on=['Data','Mandante','Visitante'],suffixes=('', f'_{i}'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_rows(df, key_columns, verbose=False):\n",
    "    \"\"\"\n",
    "    Mescla linhas de um DataFrame onde as colunas chave são iguais, \n",
    "    seguindo as regras:\n",
    "    - Se ambos valores forem NaN, manter NaN\n",
    "    - Se um valor for NaN e outro não, manter o valor não-NaN\n",
    "    - Se ambos tiverem valores diferentes, não mesclar essas linhas\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame original\n",
    "        key_columns: Lista com os nomes das colunas chave para agrupamento\n",
    "        verbose: Se True, mostra mensagens detalhadas durante o processo\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame com as linhas mescladas\n",
    "    \"\"\"\n",
    "    # Cria uma cópia para não modificar o DataFrame original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Identifica as colunas que não são chaves\n",
    "    non_key_columns = [col for col in df.columns if col not in key_columns]\n",
    "    \n",
    "    # Agrupa por colunas chave\n",
    "    grouped = result_df.groupby(key_columns)\n",
    "    \n",
    "    # Lista para armazenar as linhas mescladas\n",
    "    merged_rows = []\n",
    "    conflicts = []\n",
    "    \n",
    "    # Processa cada grupo\n",
    "    for name, group in grouped:\n",
    "        # Se o grupo tem apenas uma linha, adiciona diretamente\n",
    "        if len(group) == 1:\n",
    "            merged_rows.append(group.iloc[0])\n",
    "            continue\n",
    "        \n",
    "        # Processa grupos com múltiplas linhas\n",
    "        merged_row = {}\n",
    "        \n",
    "        # Adiciona as colunas chave\n",
    "        for i, col in enumerate(key_columns):\n",
    "            if isinstance(name, tuple):\n",
    "                merged_row[col] = name[i]\n",
    "            else:\n",
    "                merged_row[col] = name\n",
    "        \n",
    "        # Verifica conflitos nas outras colunas\n",
    "        has_conflict = False\n",
    "        \n",
    "        for col in non_key_columns:\n",
    "            # Obtém todos os valores não-NaN da coluna no grupo\n",
    "            values = group[col].dropna().unique()\n",
    "            \n",
    "            # Caso 1: Todos são NaN\n",
    "            if len(values) == 0:\n",
    "                merged_row[col] = np.nan\n",
    "            \n",
    "            # Caso 2: Apenas um valor único não-NaN\n",
    "            elif len(values) == 1:\n",
    "                merged_row[col] = values[0]\n",
    "            \n",
    "            # Caso 3: Múltiplos valores diferentes\n",
    "            else:\n",
    "                has_conflict = True\n",
    "                if verbose:\n",
    "                    print(f\"Conflito encontrado no grupo {name} na coluna {col}:\")\n",
    "                    print(f\"Valores: {values}\")\n",
    "                conflicts.append(name)\n",
    "                break\n",
    "        \n",
    "        if not has_conflict:\n",
    "            merged_rows.append(merged_row)\n",
    "        else:\n",
    "            # Se há conflito, mantém as linhas originais\n",
    "            for _, row in group.iterrows():\n",
    "                merged_rows.append(row.to_dict())\n",
    "    \n",
    "    # Cria um novo DataFrame com as linhas mescladas\n",
    "    result = pd.DataFrame(merged_rows)\n",
    "    \n",
    "    # Reordena as colunas para manter a mesma ordem do DataFrame original\n",
    "    result = result[df.columns]\n",
    "    \n",
    "    if verbose and conflicts:\n",
    "        print(f\"Total de grupos com conflitos: {len(set(conflicts))}\")\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0b8c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edition\n",
      "2006.0    751\n",
      "2004.0    730\n",
      "2003.0    661\n",
      "2021.0    589\n",
      "2005.0    585\n",
      "2010.0    584\n",
      "2022.0    575\n",
      "2018.0    568\n",
      "2019.0    563\n",
      "2020.0    562\n",
      "2017.0    551\n",
      "2016.0    530\n",
      "2023.0    522\n",
      "2013.0    521\n",
      "2015.0    506\n",
      "2014.0    497\n",
      "2012.0    478\n",
      "2007.0    461\n",
      "2024.0    423\n",
      "2008.0    391\n",
      "2009.0    388\n",
      "2011.0    380\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_14896\\2588190267.py:29: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return grupo.ffill().bfill().iloc[0]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_14896\\2588190267.py:32: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grupos = df.groupby(chaves, as_index=False).apply(mescla_segura).dropna().reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "df = df_merged.copy()\n",
    "\n",
    "# Criar edições dos campeonatos visto que 2020 foi ate 2021\n",
    "mask = (df['Data'] > dt.datetime(2020, 8, 1)) & (df['Data'] < dt.datetime(2021, 2, 28))\n",
    "df.loc[mask, 'edition'] = 2020\n",
    "df.loc[~mask, 'edition'] = df.loc[~mask, 'Data'].dt.year\n",
    "\n",
    "print(df['edition'].value_counts())\n",
    "\n",
    "df = df[df['edition'] == 2024]\n",
    "duplicados = df[df.duplicated(subset=['Mandante', 'Visitante'], keep=False)]\n",
    "#duplicados.sort_values(by=['Mandante', 'Visitante'], ascending=[True, False])\n",
    "#display(duplicados.sort_values(by='Mandante'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "7909bf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 160)\n"
     ]
    }
   ],
   "source": [
    "print(grupos.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
