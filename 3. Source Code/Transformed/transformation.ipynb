{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d025e2",
   "metadata": {},
   "source": [
    "Iniciando a uniação e validação das bases de dados extraidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c29f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "path = r'C:\\Users\\USER\\Desktop\\GitHub\\Brasileirao-Analise\\1. Data\\Transformed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "400f7589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Games \n",
    "df_list = []\n",
    "for i in range(1, 8):\n",
    "    df = pd.read_excel(path + f'\\\\0{i}_games.xlsx')\n",
    "    df_list.append(df)\n",
    "\n",
    "# df_1 = pd.read_excel(path + r'\\01_games.xlsx')\n",
    "# df_2 = pd.read_excel(path + r'\\02_games.xlsx')\n",
    "# df_3 = pd.read_excel(path + r'\\03_games.xlsx')\n",
    "# df_4 = pd.read_excel(path + r'\\04_games.xlsx')\n",
    "# df_5 = pd.read_excel(path + r'\\05_games.xlsx')\n",
    "# df_6 = pd.read_excel(path + r'\\06_games.xlsx')\n",
    "# df_7 = pd.read_excel(path + r'\\07_games.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faaf427",
   "metadata": {},
   "source": [
    "Visão geral de cada um das bases de dados, escolha dos principais atributos para avaliar público.\n",
    "\n",
    "DF2 removido, não tem identificação explicita de partida, ano! da pra fazer mas as outras bases de dados já são suficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cea704f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_id = 3  # 0 - 6\n",
    "# display(df_list[db_id].describe())\n",
    "# display(df_list[db_id].head(-3))\n",
    "# display(df_list[db_id].info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8265b580",
   "metadata": {},
   "source": [
    "Padronizar data, mandante e visitante para indexar arquivos e agrupalos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae114149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n"
     ]
    }
   ],
   "source": [
    "#fitragem série B comjumto 7\n",
    "df_list[6] = df_list[6][df_list[6]['Série']=='A']\n",
    "\n",
    "# mapeamento_colunas chave primária\n",
    "mapeamento_colunas = {\n",
    "    'data': 'Data',\n",
    "    'Date': 'Data',\n",
    "    'data_partida': 'Data',\n",
    "    'time_mandante': 'Mandante',\n",
    "    'mandante': 'Mandante',\n",
    "    'Home': 'Mandante',\n",
    "    'time_visitante': 'Visitante',\n",
    "    'visitante': 'Visitante',\n",
    "    'Away': 'Visitante',\n",
    "}\n",
    "df_list = [df.rename(columns=mapeamento_colunas) for df in df_list]\n",
    "\n",
    "# Padronizando Data\n",
    "for i in range(len(df_list)):\n",
    "    df_list[i]['Data'] = pd.to_datetime(df_list[i]['Data'], dayfirst=True, errors='coerce')\n",
    "    df_list[i] = df_list[i].sort_values(by='Data').reset_index(drop=True)\n",
    "\n",
    "mandante_list = []\n",
    "visitante_list = []\n",
    "for i in range(len(df_list)):\n",
    "    mandante_list += df_list[i]['Mandante'].unique().tolist()\n",
    "    visitante_list += df_list[i]['Visitante'].unique().tolist()\n",
    "print(len(visitante_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f0b4c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cached_download' from 'huggingface_hub' (c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01munidecode\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspellchecker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpellChecker\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.2.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset, ParallelSentencesDataset\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceTransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\datasets\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDenoisingAutoEncoderDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNoDuplicatesDataLoader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParallelSentencesDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentencesDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceLabelDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceLabelDataset\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\datasets\\ParallelSentencesDataset.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndarray\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfApi, HfFolder, Repository, hf_hub_url, cached_download\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor, device\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'cached_download' from 'huggingface_hub' (c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\__init__.py)"
     ]
    }
   ],
   "source": [
    "## agrupamento de categorias de times com nomes diferentes mas que representam o mesmo time\n",
    "## utilizando IA, e correção ortogrfica\n",
    "\n",
    "import re\n",
    "import unidecode\n",
    "from spellchecker import SpellChecker\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Etapa 1: correção ortográfica\n",
    "spell = SpellChecker(language='pt')\n",
    "\n",
    "def corrigir_erros(texto):\n",
    "    palavras = texto.split()\n",
    "    corrigidas = [spell.correction(p) or p for p in palavras]\n",
    "    return ' '.join(corrigidas)\n",
    "\n",
    "# Etapa 2: padronização (sem acento, siglas, pontuação)\n",
    "def padronizar_nome(nome):\n",
    "    nome = corrigir_erros(nome)\n",
    "    nome = unidecode.unidecode(nome.lower())\n",
    "    nome = re.sub(r'\\b(c\\.?r\\.?|clube de regatas|atl\\.?|f\\.?c\\.?|mg|sp|rj|b\\.?f\\.?r\\.?|s\\.?p\\.?f\\.?c\\.?|s\\.?c\\.?)\\b', '', nome)\n",
    "    nome = re.sub(r'[^a-z ]', '', nome)\n",
    "    nome = re.sub(r'\\s+', ' ', nome).strip()\n",
    "    return nome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df031020",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dados de exemplo\n",
    "nomes = [\n",
    "    \"Flamengo\", \"CR Flamnego\", \"Clube de Regatas Flamengo\", \"Fla\",\n",
    "    \"Atlétco-MG\", \"Atl. Mineiro\", \"Galo\", \"Atlético Mineiro\",\n",
    "    \"Botafogo\", \"Btafogo\", \"BFR\", \"Fogão\",\n",
    "    \"São Paulo\", \"SPFC\", \"Tricolor Paulista\",\n",
    "    \"Vasco da Gama\", \"CR Vasco\", \"Vascão\"\n",
    "]\n",
    "\n",
    "# Criação do DataFrame\n",
    "df = pd.DataFrame(nomes, columns=[\"original\"])\n",
    "df['limpo'] = df['original'].apply(padronizar_nome)\n",
    "\n",
    "# Etapa 3: embeddings com Sentence-BERT\n",
    "modelo = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "embeddings = modelo.encode(df['limpo'].tolist())\n",
    "\n",
    "# Função para encontrar os N nomes mais similares a uma dada consulta\n",
    "def buscar_similares(consulta, embeddings, df, top_n=5):\n",
    "    consulta_limpa = padronizar_nome(consulta)\n",
    "    consulta_embedding = modelo.encode([consulta_limpa])[0]\n",
    "\n",
    "    # Calcular a similaridade do cosseno entre a consulta e todos os nomes\n",
    "    similaridades = cosine_similarity([consulta_embedding], embeddings)[0]\n",
    "\n",
    "    # Obter os índices dos N nomes mais similares\n",
    "    indices_similares = np.argsort(similaridades)[::-1][1:top_n+1] # Exclui o primeiro (a própria consulta se estiver na lista)\n",
    "\n",
    "    # Criar um DataFrame com os resultados\n",
    "    resultados = pd.DataFrame({\n",
    "        'original': df['original'].iloc[indices_similares].tolist(),\n",
    "        'similaridade': similaridades[indices_similares]\n",
    "    })\n",
    "\n",
    "    return resultados\n",
    "\n",
    "# Exemplo de uso da função de busca semântica\n",
    "consulta = \"flamengo rj\"\n",
    "resultados = buscar_similares(consulta, embeddings, df, top_n=3)\n",
    "\n",
    "print(f\"Resultados da busca para '{consulta}':\")\n",
    "print(resultados)\n",
    "\n",
    "consulta = \"atletico mineiro futebol clube\"\n",
    "resultados = buscar_similares(consulta, embeddings, df, top_n=2)\n",
    "\n",
    "print(f\"\\nResultados da busca para '{consulta}':\")\n",
    "print(resultados)\n",
    "\n",
    "consulta = \"bfr rio\"\n",
    "resultados = buscar_similares(consulta, embeddings, df, top_n=2)\n",
    "\n",
    "print(f\"\\nResultados da busca para '{consulta}':\")\n",
    "print(resultados)\n",
    "\n",
    "consulta = \"sao paulo futebol\"\n",
    "resultados = buscar_similares(consulta, embeddings, df, top_n=2)\n",
    "\n",
    "print(f\"\\nResultados da busca para '{consulta}':\")\n",
    "print(resultados)\n",
    "\n",
    "consulta = \"vasco\"\n",
    "resultados = buscar_similares(consulta, embeddings, df, top_n=2)\n",
    "\n",
    "print(f\"\\nResultados da busca para '{consulta}':\")\n",
    "print(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e5dc9",
   "metadata": {},
   "source": [
    "🧹 1. Limpeza e padronização de texto\n",
    "Ferramentas: spaCy, transformers, ChatGPT, regex, etc.\n",
    "\n",
    "📌 Objetivo: remover ruídos (acentuação inconsistente, uso de maiúsculas/minúsculas, stopwords, espaços extras, etc.), padronizar dados para facilitar análise e comparação.\n",
    "\n",
    "\n",
    "| **Técnica**                         | **O que faz**                                              | **Exemplo**                                              |\n",
    "|-------------------------------------|-------------------------------------------------------------|----------------------------------------------------------|\n",
    "| Conversão para minúsculas           | Uniformiza o texto, evita distinções desnecessárias         | \"JOÃO da Silva\" → \"joão da silva\"                       |\n",
    "| Remoção de acentos e caracteres especiais | Facilita comparação e buscas                           | \"ação\" → \"acao\", \"João!\" → \"joao\"                      |\n",
    "| Remoção de espaços extras           | Elimina espaços duplicados                                  | \"João   Silva\" → \"João Silva\"                          |\n",
    "| Tokenização                         | Divide frases em palavras ou unidades                       | \"Rua das Palmeiras\" → [\"Rua\", \"das\", \"Palmeiras\"]         |\n",
    "| Lematização                         | Reduz palavras à forma canônica (radical)                   | \"andando\" → \"andar\"                               |\n",
    "| Remoção de stopwords                | Elimina palavras irrelevantes (ex: \"de\", \"a\", \"por\")        | \"Rua de São João\" → \"Rua São João\"                        |\n",
    "| Padronização de abreviações         | Transforma formas abreviadas em completas                   | \"Av.\" → \"Avenida\", \"R.\" → \"Rua\"                          |\n",
    "| Correção de erros comuns            | Corrige termos digitados incorretamente                     | \"Avnda Paulista\" → \"Avenida Paulista\"                    |\n",
    "| Normalização semântica              | Usa NLP para unificar sinônimos/contextos usando GPT ou BERT                 | \"Prof.\" → \"Professor\" ou “R. João Silva” → “Rua João Silva”                          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bec7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bibliotecas necessárias para limpeza e padronização\n",
    "import re\n",
    "import unidecode\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# inicializações\n",
    "# Modelo SpaCy em português\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "def para_minusculas(texto):\n",
    "    if isinstance(texto, str):\n",
    "        return texto.lower()\n",
    "    return texto\n",
    "\n",
    "def remover_acentos(texto):\n",
    "    if isinstance(texto, str):\n",
    "        return unidecode.unidecode(texto)\n",
    "    return texto\n",
    "\n",
    "def remover_caracteres_especiais(texto):\n",
    "    if isinstance(texto, str):\n",
    "        return re.sub(r'[^a-zA-Z0-9\\s]', '', texto)\n",
    "    return texto\n",
    "\n",
    "def remover_espacos_extras(texto):\n",
    "    if isinstance(texto, str):\n",
    "        return re.sub(r'\\s+', ' ', texto).strip()\n",
    "    return texto\n",
    "\n",
    "def remover_stopwords(texto):\n",
    "    if isinstance(texto, str):\n",
    "        doc = nlp(texto)\n",
    "        return ' '.join([token.text for token in doc if not token.is_stop])\n",
    "    return texto\n",
    "\n",
    "def lematizar(texto):\n",
    "    if isinstance(texto, str):\n",
    "        doc = nlp(texto)\n",
    "        return ' '.join([token.lemma_ for token in doc])\n",
    "    return texto\n",
    "\n",
    "def pipeline_avancado(texto, usar_chatgpt=False):\n",
    "    texto = para_minusculas(texto)\n",
    "    texto = remover_acentos(texto)\n",
    "    texto = remover_caracteres_especiais(texto)\n",
    "    texto = remover_espacos_extras(texto)\n",
    "\n",
    "    # NLP com SpaCy\n",
    "    texto = remover_stopwords(texto)\n",
    "    texto = lematizar(texto)\n",
    "\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "607cba28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n",
      "{'Paysandy'}\n",
      "set()\n",
      "{'Mirassol'}\n",
      "set()\n",
      "set()\n",
      "324\n"
     ]
    }
   ],
   "source": [
    "#display(df_list[0].head(1))\n",
    "#display(df_list[0][['data', 'time_mandante', 'time_visitante']])\n",
    "\n",
    "times_unicos = 0\n",
    "for i in range(len(df_list)):\n",
    "    #print(df_list[i]['Mandante'].unique())\n",
    "    #print(set(df_list[i]['Mandante'].unique()) == set(df_list[i]['Visitante'].unique())) \n",
    "    #print(df_list[i]['Mandante'].unique(),df_list[i]['Visitante'].unique())\n",
    "    times_unicos += len(df_list[i]['Mandante'].unique())\n",
    "\n",
    "#display(df_list[2]['Mandante'].unique(),df_list[2]['Visitante'].unique())\n",
    "    set1 = set(df_list[i]['Mandante'].unique())\n",
    "    set2 = set(df_list[i]['Visitante'].unique())\n",
    "\n",
    "    diferente = set1.symmetric_difference(set2)\n",
    "    print(diferente)\n",
    "print(times_unicos)\n",
    "# ==============================\n",
    "# Exemplo de uso com Pandas\n",
    "# ==============================\n",
    "# df['texto_limpo'] = df['coluna_original'].apply(lambda x: pipeline_avancado(x, usar_chatgpt=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72422928",
   "metadata": {},
   "source": [
    "📝 2. Correção ortográfica\n",
    "Ferramentas: pyspellchecker, modelos de linguagem (GPT, BERTimbau, etc.)\n",
    "\n",
    "📌 Objetivo: corrigir erros de digitação ou ortografia que afetam a análise.\n",
    "\n",
    "🔧 Técnicas:\n",
    "\n",
    "Corretores simples baseados em dicionários: como o pyspellchecker, útil para textos curtos e termos conhecidos.\n",
    "\n",
    "Modelos de linguagem (GPT, BERT): corrigem erros com base no contexto, muito úteis para frases completas ou nomes incomuns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Correção ortográfica com Transformers\n",
    "# ==============================\n",
    "def corrigir_palavra_via_ia(texto):\n",
    "    palavras = texto.split()\n",
    "    texto_corrigido = []\n",
    "\n",
    "    for palavra in palavras:\n",
    "        entrada = texto.replace(palavra, preencher_mascara.tokenizer.mask_token)\n",
    "        try:\n",
    "            sugestao = preencher_mascara(entrada)[0]['token_str']\n",
    "            texto_corrigido.append(sugestao)\n",
    "        except:\n",
    "            texto_corrigido.append(palavra)\n",
    "    return ' '.join(texto_corrigido)\n",
    "\n",
    "# ==============================\n",
    "# Correção avançada com ChatGPT\n",
    "# ==============================\n",
    "def limpar_com_chatgpt(texto):\n",
    "    prompt = f\"\"\"Revise o texto abaixo e corrija ortografia, acentuação e padronize nomes de ruas e abreviações. Retorne apenas o texto corrigido.\n",
    "Texto: \"{texto}\"\n",
    "Corrigido:\"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4810aef2",
   "metadata": {},
   "source": [
    "🧩 3. Preenchimento de valores ausentes (missing values)\n",
    "Ferramentas: sklearn (KNNImputer, regressão), fancyimpute, modelos supervisionados.\n",
    "\n",
    "📌 Objetivo: evitar perda de dados substituindo valores ausentes com estimativas coerentes.\n",
    "\n",
    "🔧 Técnicas comuns:\n",
    "\n",
    "KNNImputer: preenche com base em vizinhos mais próximos.\n",
    "\n",
    "Regressão/MICE: usa relações entre variáveis para prever os valores faltantes.\n",
    "\n",
    "Modelos supervisionados: predição usando modelos customizados (ex: árvore de decisão para prever “salário” com base em “idade”, “formação”, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c95d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74c99689",
   "metadata": {},
   "source": [
    "🚨 4. Detecção de outliers\n",
    "Ferramentas: Isolation Forest, DBSCAN, Autoencoders, Z-score, IQR.\n",
    "\n",
    "📌 Objetivo: identificar e tratar valores anômalos que podem distorcer análises.\n",
    "\n",
    "🔧 Técnicas:\n",
    "\n",
    "Isolation Forest: modelo de árvore que isola outliers com base em caminhos curtos.\n",
    "\n",
    "DBSCAN: algoritmo de cluster que separa ruído.\n",
    "\n",
    "Autoencoders: redes neurais que aprendem padrões normais e destacam desvios.\n",
    "\n",
    "Z-score/IQR: métodos estatísticos clássicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4fd32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff840945",
   "metadata": {},
   "source": [
    "🔁 5. Deduplicação e fuzzy matching\n",
    "Ferramentas: fuzzywuzzy, Levenshtein, sklearn (TF-IDF), Hugging Face models.\n",
    "\n",
    "📌 Objetivo: detectar registros duplicados com variações pequenas (ex: “João da Silva” e “J. Silva”).\n",
    "\n",
    "🔧 Técnicas:\n",
    "\n",
    "Fuzzy Matching com Levenshtein (distância de edição).\n",
    "\n",
    "TF-IDF + KMeans/Agglomerative Clustering para comparar textos.\n",
    "\n",
    "Modelos como Sentence-BERT: usam embeddings semânticos para comparar significado das frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77bc7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f25f2557",
   "metadata": {},
   "source": [
    "📊 6. Enriquecimento de dados\n",
    "Ferramentas: APIs externas, embeddings semânticos, web scraping com IA.\n",
    "\n",
    "📌 Objetivo: adicionar contexto e informações adicionais úteis a partir de fontes externas.\n",
    "\n",
    "🔧 Técnicas:\n",
    "\n",
    "APIs de geolocalização: completar endereços, CEPs, coordenadas.\n",
    "\n",
    "Modelos de NLP: classificar textos, extrair categorias, prever sentimento.\n",
    "\n",
    "Web scraping com IA: extrair dados de sites automaticamente.\n",
    "\n",
    "Embeddings: transformar texto em vetores para inferir similaridades e contextos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa32bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
