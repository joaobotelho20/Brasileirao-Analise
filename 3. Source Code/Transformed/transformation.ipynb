{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d025e2",
   "metadata": {},
   "source": [
    "Iniciando a unia√ß√£o e valida√ß√£o das bases de dados extraidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c29f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "path = r'C:\\Users\\USER\\Desktop\\GitHub\\Brasileirao-Analise\\1. Data\\Transformed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "400f7589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Games \n",
    "df_list = []\n",
    "for i in range(1, 8):\n",
    "    df = pd.read_excel(path + f'\\\\0{i}_games.xlsx')\n",
    "    df_list.append(df)\n",
    "\n",
    "# df_1 = pd.read_excel(path + r'\\01_games.xlsx')\n",
    "# df_2 = pd.read_excel(path + r'\\02_games.xlsx')\n",
    "# df_3 = pd.read_excel(path + r'\\03_games.xlsx')\n",
    "# df_4 = pd.read_excel(path + r'\\04_games.xlsx')\n",
    "# df_5 = pd.read_excel(path + r'\\05_games.xlsx')\n",
    "# df_6 = pd.read_excel(path + r'\\06_games.xlsx')\n",
    "# df_7 = pd.read_excel(path + r'\\07_games.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faaf427",
   "metadata": {},
   "source": [
    "Vis√£o geral de cada um das bases de dados, escolha dos principais atributos para avaliar p√∫blico.\n",
    "\n",
    "DF2 removido, n√£o tem identifica√ß√£o explicita de partida, ano! da pra fazer mas as outras bases de dados j√° s√£o suficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cea704f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_id = 3  # 0 - 6\n",
    "# display(df_list[db_id].describe())\n",
    "# display(df_list[db_id].head(-3))\n",
    "# display(df_list[db_id].info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8265b580",
   "metadata": {},
   "source": [
    "Padronizar data, mandante e visitante para indexar arquivos e agrupalos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae114149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n"
     ]
    }
   ],
   "source": [
    "#fitragem s√©rie B comjumto 7\n",
    "df_list[6] = df_list[6][df_list[6]['S√©rie']=='A']\n",
    "\n",
    "# mapeamento_colunas chave prim√°ria\n",
    "mapeamento_colunas = {\n",
    "    'data': 'Data',\n",
    "    'Date': 'Data',\n",
    "    'data_partida': 'Data',\n",
    "    'time_mandante': 'Mandante',\n",
    "    'mandante': 'Mandante',\n",
    "    'Home': 'Mandante',\n",
    "    'time_visitante': 'Visitante',\n",
    "    'visitante': 'Visitante',\n",
    "    'Away': 'Visitante',\n",
    "}\n",
    "df_list = [df.rename(columns=mapeamento_colunas) for df in df_list]\n",
    "\n",
    "# Padronizando Data\n",
    "for i in range(len(df_list)):\n",
    "    df_list[i]['Data'] = pd.to_datetime(df_list[i]['Data'], dayfirst=True, errors='coerce')\n",
    "    df_list[i] = df_list[i].sort_values(by='Data').reset_index(drop=True)\n",
    "\n",
    "mandante_list = []\n",
    "visitante_list = []\n",
    "for i in range(len(df_list)):\n",
    "    mandante_list += df_list[i]['Mandante'].unique().tolist()\n",
    "    visitante_list += df_list[i]['Visitante'].unique().tolist()\n",
    "print(len(visitante_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f0b4c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cached_download' from 'huggingface_hub' (c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01munidecode\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspellchecker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpellChecker\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.2.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset, ParallelSentencesDataset\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceTransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\datasets\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDenoisingAutoEncoderDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNoDuplicatesDataLoader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParallelSentencesDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentencesDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceLabelDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceLabelDataset\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\datasets\\ParallelSentencesDataset.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndarray\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfApi, HfFolder, Repository, hf_hub_url, cached_download\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor, device\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'cached_download' from 'huggingface_hub' (c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\__init__.py)"
     ]
    }
   ],
   "source": [
    "## agrupamento de categorias de times com nomes diferentes mas que representam o mesmo time\n",
    "## utilizando IA, e corre√ß√£o ortogrfica\n",
    "\n",
    "import re\n",
    "import unidecode\n",
    "from spellchecker import SpellChecker\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Etapa 1: corre√ß√£o ortogr√°fica\n",
    "spell = SpellChecker(language='pt')\n",
    "\n",
    "def corrigir_erros(texto):\n",
    "    palavras = texto.split()\n",
    "    corrigidas = [spell.correction(p) or p for p in palavras]\n",
    "    return ' '.join(corrigidas)\n",
    "\n",
    "# Etapa 2: padroniza√ß√£o (sem acento, siglas, pontua√ß√£o)\n",
    "def padronizar_nome(nome):\n",
    "    nome = corrigir_erros(nome)\n",
    "    nome = unidecode.unidecode(nome.lower())\n",
    "    nome = re.sub(r'\\b(c\\.?r\\.?|clube de regatas|atl\\.?|f\\.?c\\.?|mg|sp|rj|b\\.?f\\.?r\\.?|s\\.?p\\.?f\\.?c\\.?|s\\.?c\\.?)\\b', '', nome)\n",
    "    nome = re.sub(r'[^a-z ]', '', nome)\n",
    "    nome = re.sub(r'\\s+', ' ', nome).strip()\n",
    "    return nome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df031020",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dados de exemplo\n",
    "nomes = [\n",
    "    \"Flamengo\", \"CR Flamnego\", \"Clube de Regatas Flamengo\", \"Fla\",\n",
    "    \"Atl√©tco-MG\", \"Atl. Mineiro\", \"Galo\", \"Atl√©tico Mineiro\",\n",
    "    \"Botafogo\", \"Btafogo\", \"BFR\", \"Fog√£o\",\n",
    "    \"S√£o Paulo\", \"SPFC\", \"Tricolor Paulista\",\n",
    "    \"Vasco da Gama\", \"CR Vasco\", \"Vasc√£o\"\n",
    "]\n",
    "\n",
    "# Cria√ß√£o do DataFrame\n",
    "df = pd.DataFrame(nomes, columns=[\"original\"])\n",
    "df['limpo'] = df['original'].apply(padronizar_nome)\n",
    "\n",
    "# Etapa 3: embeddings com Sentence-BERT\n",
    "modelo = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "embeddings = modelo.encode(df['limpo'].tolist())\n",
    "\n",
    "# Fun√ß√£o para encontrar os N nomes mais similares a uma dada consulta\n",
    "def buscar_similares(consulta, embeddings, df, top_n=5):\n",
    "    consulta_limpa = padronizar_nome(consulta)\n",
    "    consulta_embedding = modelo.encode([consulta_limpa])[0]\n",
    "\n",
    "    # Calcular a similaridade do cosseno entre a consulta e todos os nomes\n",
    "    similaridades = cosine_similarity([consulta_embedding], embeddings)[0]\n",
    "\n",
    "    # Obter os √≠ndices dos N nomes mais similares\n",
    "    indices_similares = np.argsort(similaridades)[::-1][1:top_n+1] # Exclui o primeiro (a pr√≥pria consulta se estiver na lista)\n",
    "\n",
    "    # Criar um DataFrame com os resultados\n",
    "    resultados = pd.DataFrame({\n",
    "        'original': df['original'].iloc[indices_similares].tolist(),\n",
    "        'similaridade': similaridades[indices_similares]\n",
    "    })\n",
    "\n",
    "    return resultados\n",
    "\n",
    "# Exemplo de uso da fun√ß√£o de busca sem√¢ntica\n",
    "consulta = \"flamengo rj\"\n",
    "resultados = buscar_similares(consulta, embeddings, df, top_n=3)\n",
    "\n",
    "print(f\"Resultados da busca para '{consulta}':\")\n",
    "print(resultados)\n",
    "\n",
    "consulta = \"atletico mineiro futebol clube\"\n",
    "resultados = buscar_similares(consulta, embeddings, df, top_n=2)\n",
    "\n",
    "print(f\"\\nResultados da busca para '{consulta}':\")\n",
    "print(resultados)\n",
    "\n",
    "consulta = \"bfr rio\"\n",
    "resultados = buscar_similares(consulta, embeddings, df, top_n=2)\n",
    "\n",
    "print(f\"\\nResultados da busca para '{consulta}':\")\n",
    "print(resultados)\n",
    "\n",
    "consulta = \"sao paulo futebol\"\n",
    "resultados = buscar_similares(consulta, embeddings, df, top_n=2)\n",
    "\n",
    "print(f\"\\nResultados da busca para '{consulta}':\")\n",
    "print(resultados)\n",
    "\n",
    "consulta = \"vasco\"\n",
    "resultados = buscar_similares(consulta, embeddings, df, top_n=2)\n",
    "\n",
    "print(f\"\\nResultados da busca para '{consulta}':\")\n",
    "print(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e5dc9",
   "metadata": {},
   "source": [
    "üßπ 1. Limpeza e padroniza√ß√£o de texto\n",
    "Ferramentas: spaCy, transformers, ChatGPT, regex, etc.\n",
    "\n",
    "üìå Objetivo: remover ru√≠dos (acentua√ß√£o inconsistente, uso de mai√∫sculas/min√∫sculas, stopwords, espa√ßos extras, etc.), padronizar dados para facilitar an√°lise e compara√ß√£o.\n",
    "\n",
    "\n",
    "| **T√©cnica**                         | **O que faz**                                              | **Exemplo**                                              |\n",
    "|-------------------------------------|-------------------------------------------------------------|----------------------------------------------------------|\n",
    "| Convers√£o para min√∫sculas           | Uniformiza o texto, evita distin√ß√µes desnecess√°rias         | \"JO√ÉO da Silva\" ‚Üí \"jo√£o da silva\"                       |\n",
    "| Remo√ß√£o de acentos e caracteres especiais | Facilita compara√ß√£o e buscas                           | \"a√ß√£o\" ‚Üí \"acao\", \"Jo√£o!\" ‚Üí \"joao\"                      |\n",
    "| Remo√ß√£o de espa√ßos extras           | Elimina espa√ßos duplicados                                  | \"Jo√£o   Silva\" ‚Üí \"Jo√£o Silva\"                          |\n",
    "| Tokeniza√ß√£o                         | Divide frases em palavras ou unidades                       | \"Rua das Palmeiras\" ‚Üí [\"Rua\", \"das\", \"Palmeiras\"]         |\n",
    "| Lematiza√ß√£o                         | Reduz palavras √† forma can√¥nica (radical)                   | \"andando\" ‚Üí \"andar\"                               |\n",
    "| Remo√ß√£o de stopwords                | Elimina palavras irrelevantes (ex: \"de\", \"a\", \"por\")        | \"Rua de S√£o Jo√£o\" ‚Üí \"Rua S√£o Jo√£o\"                        |\n",
    "| Padroniza√ß√£o de abrevia√ß√µes         | Transforma formas abreviadas em completas                   | \"Av.\" ‚Üí \"Avenida\", \"R.\" ‚Üí \"Rua\"                          |\n",
    "| Corre√ß√£o de erros comuns            | Corrige termos digitados incorretamente                     | \"Avnda Paulista\" ‚Üí \"Avenida Paulista\"                    |\n",
    "| Normaliza√ß√£o sem√¢ntica              | Usa NLP para unificar sin√¥nimos/contextos usando GPT ou BERT                 | \"Prof.\" ‚Üí \"Professor\" ou ‚ÄúR. Jo√£o Silva‚Äù ‚Üí ‚ÄúRua Jo√£o Silva‚Äù                          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bec7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bibliotecas necess√°rias para limpeza e padroniza√ß√£o\n",
    "import re\n",
    "import unidecode\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# inicializa√ß√µes\n",
    "# Modelo SpaCy em portugu√™s\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "def para_minusculas(texto):\n",
    "    if isinstance(texto, str):\n",
    "        return texto.lower()\n",
    "    return texto\n",
    "\n",
    "def remover_acentos(texto):\n",
    "    if isinstance(texto, str):\n",
    "        return unidecode.unidecode(texto)\n",
    "    return texto\n",
    "\n",
    "def remover_caracteres_especiais(texto):\n",
    "    if isinstance(texto, str):\n",
    "        return re.sub(r'[^a-zA-Z0-9\\s]', '', texto)\n",
    "    return texto\n",
    "\n",
    "def remover_espacos_extras(texto):\n",
    "    if isinstance(texto, str):\n",
    "        return re.sub(r'\\s+', ' ', texto).strip()\n",
    "    return texto\n",
    "\n",
    "def remover_stopwords(texto):\n",
    "    if isinstance(texto, str):\n",
    "        doc = nlp(texto)\n",
    "        return ' '.join([token.text for token in doc if not token.is_stop])\n",
    "    return texto\n",
    "\n",
    "def lematizar(texto):\n",
    "    if isinstance(texto, str):\n",
    "        doc = nlp(texto)\n",
    "        return ' '.join([token.lemma_ for token in doc])\n",
    "    return texto\n",
    "\n",
    "def pipeline_avancado(texto, usar_chatgpt=False):\n",
    "    texto = para_minusculas(texto)\n",
    "    texto = remover_acentos(texto)\n",
    "    texto = remover_caracteres_especiais(texto)\n",
    "    texto = remover_espacos_extras(texto)\n",
    "\n",
    "    # NLP com SpaCy\n",
    "    texto = remover_stopwords(texto)\n",
    "    texto = lematizar(texto)\n",
    "\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "607cba28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n",
      "{'Paysandy'}\n",
      "set()\n",
      "{'Mirassol'}\n",
      "set()\n",
      "set()\n",
      "324\n"
     ]
    }
   ],
   "source": [
    "#display(df_list[0].head(1))\n",
    "#display(df_list[0][['data', 'time_mandante', 'time_visitante']])\n",
    "\n",
    "times_unicos = 0\n",
    "for i in range(len(df_list)):\n",
    "    #print(df_list[i]['Mandante'].unique())\n",
    "    #print(set(df_list[i]['Mandante'].unique()) == set(df_list[i]['Visitante'].unique())) \n",
    "    #print(df_list[i]['Mandante'].unique(),df_list[i]['Visitante'].unique())\n",
    "    times_unicos += len(df_list[i]['Mandante'].unique())\n",
    "\n",
    "#display(df_list[2]['Mandante'].unique(),df_list[2]['Visitante'].unique())\n",
    "    set1 = set(df_list[i]['Mandante'].unique())\n",
    "    set2 = set(df_list[i]['Visitante'].unique())\n",
    "\n",
    "    diferente = set1.symmetric_difference(set2)\n",
    "    print(diferente)\n",
    "print(times_unicos)\n",
    "# ==============================\n",
    "# Exemplo de uso com Pandas\n",
    "# ==============================\n",
    "# df['texto_limpo'] = df['coluna_original'].apply(lambda x: pipeline_avancado(x, usar_chatgpt=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72422928",
   "metadata": {},
   "source": [
    "üìù 2. Corre√ß√£o ortogr√°fica\n",
    "Ferramentas: pyspellchecker, modelos de linguagem (GPT, BERTimbau, etc.)\n",
    "\n",
    "üìå Objetivo: corrigir erros de digita√ß√£o ou ortografia que afetam a an√°lise.\n",
    "\n",
    "üîß T√©cnicas:\n",
    "\n",
    "Corretores simples baseados em dicion√°rios: como o pyspellchecker, √∫til para textos curtos e termos conhecidos.\n",
    "\n",
    "Modelos de linguagem (GPT, BERT): corrigem erros com base no contexto, muito √∫teis para frases completas ou nomes incomuns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Corre√ß√£o ortogr√°fica com Transformers\n",
    "# ==============================\n",
    "def corrigir_palavra_via_ia(texto):\n",
    "    palavras = texto.split()\n",
    "    texto_corrigido = []\n",
    "\n",
    "    for palavra in palavras:\n",
    "        entrada = texto.replace(palavra, preencher_mascara.tokenizer.mask_token)\n",
    "        try:\n",
    "            sugestao = preencher_mascara(entrada)[0]['token_str']\n",
    "            texto_corrigido.append(sugestao)\n",
    "        except:\n",
    "            texto_corrigido.append(palavra)\n",
    "    return ' '.join(texto_corrigido)\n",
    "\n",
    "# ==============================\n",
    "# Corre√ß√£o avan√ßada com ChatGPT\n",
    "# ==============================\n",
    "def limpar_com_chatgpt(texto):\n",
    "    prompt = f\"\"\"Revise o texto abaixo e corrija ortografia, acentua√ß√£o e padronize nomes de ruas e abrevia√ß√µes. Retorne apenas o texto corrigido.\n",
    "Texto: \"{texto}\"\n",
    "Corrigido:\"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4810aef2",
   "metadata": {},
   "source": [
    "üß© 3. Preenchimento de valores ausentes (missing values)\n",
    "Ferramentas: sklearn (KNNImputer, regress√£o), fancyimpute, modelos supervisionados.\n",
    "\n",
    "üìå Objetivo: evitar perda de dados substituindo valores ausentes com estimativas coerentes.\n",
    "\n",
    "üîß T√©cnicas comuns:\n",
    "\n",
    "KNNImputer: preenche com base em vizinhos mais pr√≥ximos.\n",
    "\n",
    "Regress√£o/MICE: usa rela√ß√µes entre vari√°veis para prever os valores faltantes.\n",
    "\n",
    "Modelos supervisionados: predi√ß√£o usando modelos customizados (ex: √°rvore de decis√£o para prever ‚Äúsal√°rio‚Äù com base em ‚Äúidade‚Äù, ‚Äúforma√ß√£o‚Äù, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c95d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74c99689",
   "metadata": {},
   "source": [
    "üö® 4. Detec√ß√£o de outliers\n",
    "Ferramentas: Isolation Forest, DBSCAN, Autoencoders, Z-score, IQR.\n",
    "\n",
    "üìå Objetivo: identificar e tratar valores an√¥malos que podem distorcer an√°lises.\n",
    "\n",
    "üîß T√©cnicas:\n",
    "\n",
    "Isolation Forest: modelo de √°rvore que isola outliers com base em caminhos curtos.\n",
    "\n",
    "DBSCAN: algoritmo de cluster que separa ru√≠do.\n",
    "\n",
    "Autoencoders: redes neurais que aprendem padr√µes normais e destacam desvios.\n",
    "\n",
    "Z-score/IQR: m√©todos estat√≠sticos cl√°ssicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4fd32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff840945",
   "metadata": {},
   "source": [
    "üîÅ 5. Deduplica√ß√£o e fuzzy matching\n",
    "Ferramentas: fuzzywuzzy, Levenshtein, sklearn (TF-IDF), Hugging Face models.\n",
    "\n",
    "üìå Objetivo: detectar registros duplicados com varia√ß√µes pequenas (ex: ‚ÄúJo√£o da Silva‚Äù e ‚ÄúJ. Silva‚Äù).\n",
    "\n",
    "üîß T√©cnicas:\n",
    "\n",
    "Fuzzy Matching com Levenshtein (dist√¢ncia de edi√ß√£o).\n",
    "\n",
    "TF-IDF + KMeans/Agglomerative Clustering para comparar textos.\n",
    "\n",
    "Modelos como Sentence-BERT: usam embeddings sem√¢nticos para comparar significado das frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77bc7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f25f2557",
   "metadata": {},
   "source": [
    "üìä 6. Enriquecimento de dados\n",
    "Ferramentas: APIs externas, embeddings sem√¢nticos, web scraping com IA.\n",
    "\n",
    "üìå Objetivo: adicionar contexto e informa√ß√µes adicionais √∫teis a partir de fontes externas.\n",
    "\n",
    "üîß T√©cnicas:\n",
    "\n",
    "APIs de geolocaliza√ß√£o: completar endere√ßos, CEPs, coordenadas.\n",
    "\n",
    "Modelos de NLP: classificar textos, extrair categorias, prever sentimento.\n",
    "\n",
    "Web scraping com IA: extrair dados de sites automaticamente.\n",
    "\n",
    "Embeddings: transformar texto em vetores para inferir similaridades e contextos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa32bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
